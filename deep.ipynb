{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a simple deep neural network for solving the polynomial y = 5x^3 - 10x^2 - 20x + 10 with the following specifications:\n",
    "\n",
    "- **Use three hidden-layers of sizes 32, 64, and 128 and display the generated DNN with the required number of parameters.**\n",
    "- **Generate training samples within the range of -20 to +20. Use the appropriate method for normalizing the training data in the range of -1 to +1.**\n",
    "- **Use 5% of the samples as test data and 5% of the samples as validation data and the rest of the data for training the DNN with and appropriate number of epochs.**\n",
    "- **Display the training accuracy vs validation accuracy and training error vs validation error curves.**\n",
    "- **After training, use the test data for prediction and display the prediction accuracy vs true levels of the test data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a simple deep neural network to solve the polynomial \\( y = 5x^3 - 10x^2 - 20x + 10 \\) involves designing a network architecture with input, hidden, and output layers to learn the polynomial function's mapping from \\( x \\) to \\( y \\). The model will be trained on a dataset of \\( x \\) values and their corresponding \\( y \\) values calculated from the polynomial equation. After training, the network can predict outputs for new \\( x \\) inputs, demonstrating its ability to approximate the polynomial function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deep neural network (DNN) is designed with three hidden layers, consisting of 32, 64, and 128 neurons respectively, allowing it to learn complex representations of the input data. Each hidden layer uses an activation function (such as ReLU) to introduce non-linearity, enhancing the network's ability to model the polynomial function. Displaying the generated DNN includes the architecture details, layer types, and total number of parameters, which indicates the complexity and capacity of the model for learning the target function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating training samples within the range of -20 to +20 involves creating input values \\( x \\) from this specified interval to feed into the neural network. Normalizing the training data to the range of -1 to +1 means scaling the \\( x \\) values so that they fit within this new range, which helps improve the training stability and performance of the neural network. This normalization is typically achieved using the formula \\( x_{normalized} = \\frac{x}{\\max(|x|)} \\), ensuring that the data is centered and more conducive for efficient learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 5% of the samples as test data means that this portion will be set aside to evaluate the model's performance on unseen data after training is complete. Similarly, allocating 5% of the samples for validation data allows for monitoring the model's performance during training and tuning hyperparameters without overfitting. The remaining 90% of the samples will be used for training the deep neural network (DNN), and an appropriate number of epochs will be determined to ensure sufficient training while avoiding underfitting or overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the training accuracy versus validation accuracy curves allows for visual comparison of how well the model learns from the training data compared to its performance on the validation set during the training process. Similarly, plotting training error versus validation error curves provides insights into the model's performance by illustrating the difference in error rates over epochs, helping identify potential overfitting or underfitting. Analyzing these curves together enables better understanding of the model's learning behavior and guides adjustments to improve overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, using the test data for prediction involves feeding the unseen \\( x \\) values into the trained deep neural network to generate corresponding \\( y \\) predictions. Displaying the prediction accuracy versus true levels of the test data visually compares the model's predicted outputs against the actual values, highlighting how well the model generalizes to new data. This comparison helps assess the effectiveness of the training process and the model's overall performance in accurately approximating the polynomial function."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
